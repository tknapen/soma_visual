{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, subprocess, glob, math\n",
    "\n",
    "import pprint, h5py\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import cortex as cx\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.glm.first_level.hemodynamic_models import _gamma_difference_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/knapen/projects/soma_visual/data/derivatives/fmriprep/'\n",
    "beh_dir = '/Users/knapen/projects/soma_visual/data/sourcedata_nordic/'\n",
    "op_dir = '/Users/knapen/projects/soma_visual/data/derivatives/glms/'\n",
    "\n",
    "subject = 'sub-06'\n",
    "# visual-motor, visual-only, and auditory-motor\n",
    "conditions = ['VM', 'VO', 'AM']\n",
    "\n",
    "nr_trs_dropped = 4\n",
    "nr_acompcorrs = 4\n",
    "exclude_bilateral_regressors = True\n",
    "\n",
    "space_string = 'space-fsLR_den-170k_bold.dtseries.nii'\n",
    "\n",
    "runs = [glob.glob(os.path.join(base_dir, subject, 'func', f'{subject}_task-{condition}*{space_string}')) for condition in conditions]\n",
    "for run in runs:\n",
    "    run.sort()\n",
    "    if len(run) > 0:\n",
    "        run.pop(0)\n",
    "# pprint.pprint(runs)\n",
    "\n",
    "confound_runs = [glob.glob(os.path.join(base_dir, subject, 'func', f'{subject}_task-{condition}_*desc-confounds_timeseries.tsv')) for condition in conditions]\n",
    "for run in confound_runs:\n",
    "    run.sort()\n",
    "    if len(run) > 0:\n",
    "        run.pop(0)\n",
    "# pprint.pprint(confound_runs)\n",
    "\n",
    "event_runs = [glob.glob(os.path.join(beh_dir, subject, 'func', f'{subject}_*_task-{condition}_events.tsv')) for condition in conditions]\n",
    "for run in event_runs:\n",
    "    run.sort()\n",
    "    if len(run) > 0:\n",
    "        run.pop(0)\n",
    "# pprint.pprint(event_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_condition = 'VO'\n",
    "\n",
    "# read data, drop first 4 TRs which were dummies\n",
    "single_timecourses = np.array([nb.load(run).get_fdata()[nr_trs_dropped:] for run in runs[conditions.index(this_condition)]])\n",
    "single_timecourses -= sp.signal.savgol_filter(single_timecourses, window_length=single_timecourses.shape[1]-1, polyorder=2, axis=1)\n",
    "single_mean_epi = single_timecourses[:,5:15].mean(1)[:,np.newaxis,:]\n",
    "single_psc_data = np.nan_to_num(100 * (single_timecourses - single_mean_epi) / single_mean_epi)\n",
    "\n",
    "# average across runs\n",
    "psc_data = np.median(single_psc_data, axis=0)\n",
    "\n",
    "tr = 1.6\n",
    "n_timepoints = psc_data.shape[0]\n",
    "tr_timepoints = np.arange(tr/2,n_timepoints*tr, tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dfs = [pd.read_csv(run, sep='\\t') for run in event_runs[conditions.index(this_condition)]]\n",
    "\n",
    "confound_dfs = [pd.read_csv(run, sep='\\t')[nr_trs_dropped:].set_index(tr_timepoints) for run in confound_runs[conditions.index(this_condition)]]\n",
    "confound_dfs_z = [(cdf-cdf.mean(0))/cdf.std(0) for cdf in confound_dfs]\n",
    "\n",
    "f, axs = plt.subplots(1, len(confound_dfs), figsize=(20,5))\n",
    "for i in range(len(confound_dfs)):\n",
    "    sns.heatmap(confound_dfs_z[i], ax=axs[i], vmin=-8, vmax=8, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_columns = ['cosine00'] + [f'a_comp_cor_{str(i).zfill(2)}' for i in range(nr_acompcorrs)]\n",
    "\n",
    "confound_dms = [cdfz[confound_columns] for cdfz in confound_dfs_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confound_dms[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_event_data(edf):\n",
    "    ts = edf.response == 't'\n",
    "    start_time = edf[ts].onset_abs.values[0]\n",
    "\n",
    "    trials = ~edf.stimulus.isnull()\n",
    "    stims = edf.event_type == 'stim'\n",
    "    stim_trials = edf[trials & stims]\n",
    "    stim_trials['net_onsets'] = stim_trials.onset_abs-start_time\n",
    "\n",
    "    return stim_trials\n",
    "\n",
    "event_data = [get_relevant_event_data(edf) for edf in event_dfs]\n",
    "\n",
    "onsets = np.array([ed.net_onsets.values for ed in event_data])\n",
    "plt.plot(onsets.var(0), label='timing variance across runs')\n",
    "plt.plot(np.ones(onsets.shape[1])*1/120, label='display frame times')\n",
    "plt.xlabel('trials')\n",
    "plt.ylabel('duration [s]');\n",
    "plt.legend()\n",
    "\n",
    "np.unique(event_data[0].stimulus)\n",
    "\n",
    "for edf in event_data:\n",
    "    edf['mean_onset'] = onsets.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_event_types = ['eyebrows', 'eyes', 'mouth', 'tongue', 'lhand_fing1', 'lhand_fing2', 'lhand_fing3', 'lhand_fing4', 'lhand_fing5', 'lleg',  'rhand_fing1', 'rhand_fing2', 'rhand_fing3',\n",
    " 'rhand_fing4', 'rhand_fing5', 'rleg', 'bhand_fing1', 'bhand_fing2', 'bhand_fing3', 'bhand_fing4', 'bhand_fing5', 'bleg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, now that we have event times, we'll be able to create a fine-timing timecourse. \n",
    "# I will tell you a secret: the TR in this experiment was 1.6s - you can find this info\n",
    "# in the figshare website also. \n",
    "\n",
    "\n",
    "def create_BOLD_regressor(event_times, n_timepoints, tr=1.6, supersampleratio=10):\n",
    "\n",
    "    # create 0.1 s timescale regressor, and fill in the events\n",
    "    upscaled_times = np.arange(0, n_timepoints*tr, 1/supersampleratio)\n",
    "    neural_event_timecourse = np.zeros(int(n_timepoints*tr*supersampleratio))\n",
    "    # we'll pretend the events were instantaneous, since this will not\n",
    "    # impact the expected response shapes much. For events with variable durations\n",
    "    # you'll want to make sure this doesn't happen :-)\n",
    "    neural_event_timecourse[np.round(event_times * supersampleratio).astype(int)] = 1 \n",
    "\n",
    "    hrf = _gamma_difference_hrf(tr=tr ,oversampling=tr*supersampleratio, onset=-tr/2)\n",
    "    hrf /= hrf.max() # <- normalize HRF\n",
    "\n",
    "    bold_event_timecourse = np.convolve(neural_event_timecourse, hrf, 'full')[:neural_event_timecourse.shape[0]]\n",
    "    sub_sampled_regressor = bold_event_timecourse[::int(tr*supersampleratio)]\n",
    "    return sub_sampled_regressor\n",
    "\n",
    "def create_design_matrix(regressor_types, expt_df, data):\n",
    "    regressors = [np.ones(data.shape[1])]\n",
    "    for regressor_type in regressor_types:\n",
    "        event_times = np.array(expt_df[expt_df['stimulus'] == regressor_type]['mean_onset'])\n",
    "        regressors.append(create_BOLD_regressor(event_times, data.shape[1]))\n",
    "    return np.array(regressors)\n",
    "\n",
    "def run_glm(run_data, run_dm):\n",
    "    betas, _, _, _ = np.linalg.lstsq(run_dm, run_data)\n",
    "    betas = pd.DataFrame(betas.T, columns=run_dm.columns)\n",
    "\n",
    "    yhat = pd.DataFrame(np.dot(betas, run_dm.T).T, index=run_dm.index)\n",
    "    errors =  pd.DataFrame((yhat.T-run_data.T).T, index=run_dm.index)\n",
    "    \n",
    "    sse = np.sum(np.nan_to_num(errors) ** 2, axis=0)\n",
    "    rsq = 1-errors.var(axis=0)/run_data.var(axis=0)\n",
    "\n",
    "    sse_rsq = pd.DataFrame(np.array([sse, rsq]).T, columns=['sse', 'rsq'])\n",
    "    \n",
    "    return betas, sse_rsq, yhat, errors\n",
    "\n",
    "dm = create_design_matrix(regressor_types=unique_event_types, expt_df=event_data[0], data=psc_data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,6):\n",
    "    to_add = dm[unique_event_types.index(f'bhand_fing{x}')+1]\n",
    "    dm[unique_event_types.index(f'lhand_fing{x}')+1] += to_add\n",
    "    dm[unique_event_types.index(f'rhand_fing{x}')+1] += to_add\n",
    "\n",
    "# leg is only one\n",
    "to_add = dm[unique_event_types.index('bleg')+1]\n",
    "dm[unique_event_types.index('lleg')+1] += to_add\n",
    "dm[unique_event_types.index('rleg')+1] += to_add\n",
    "\n",
    "print(dm.shape)\n",
    "sns.heatmap(dm)\n",
    "\n",
    "if exclude_bilateral_regressors:\n",
    "#     update dm & unique event types list to exclude the bilateral responses now\n",
    "    dm = dm[:-6]\n",
    "    unique_event_types = unique_event_types[:-6]\n",
    "\n",
    "dm_df = pd.DataFrame(dm.T, columns=np.r_[['intercept'], unique_event_types], index=tr_timepoints)\n",
    "sns.heatmap(dm_df)\n",
    "\n",
    "\n",
    "dm_df_confs = [dm_df for cdm in confound_dms]\n",
    "\n",
    "# dm_df_confs = [pd.concat([dm_df, cdm], axis=1) for cdm in confound_dms]\n",
    "# f, axs = plt.subplots(1, len(dm_df_confs), figsize=(20,5))\n",
    "# for i in range(len(dm_df_confs)):\n",
    "#     sns.heatmap(dm_df_confs[i], ax=axs[i], vmin=-8, vmax=8, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(len(runs[conditions.index(this_condition)])):\n",
    "    run_data = pd.DataFrame(single_psc_data[run], index=tr_timepoints)\n",
    "    run_dm = dm_df_confs[run]\n",
    "        \n",
    "    betas, sse_rsq, yhat, errors = run_glm(run_data, run_dm)\n",
    "    \n",
    "    opf_base = os.path.join(op_dir, subject, 'func', os.path.split(runs[conditions.index(this_condition)][run])[1].replace('.dtseries.nii', ''))\n",
    "    \n",
    "    betas.to_hdf(opf_base + '_betas.h5', key='betas', complevel=6)\n",
    "    sse_rsq.to_hdf(opf_base + '_sse_rsq.h5', key='sse_rsq', complevel=6)\n",
    "    yhat.to_hdf(opf_base + '_yhat.h5', key='yhat', complevel=6)    \n",
    "    errors.to_hdf(opf_base + '_errors.h5', key='errors', complevel=6)\n",
    "    run_dm.to_hdf(opf_base + '_dm.h5', key='run_dm', complevel=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rhands = np.array([1 if 'rhand' in uet else 0 for uet in unique_event_types])\n",
    "# lhands = np.array([-1 if 'lhand' in uet else 0 for uet in unique_event_types])\n",
    "# contrast_vector = np.r_[0, rhands+lhands] # the intercept regressor was never in the event types list\n",
    "\n",
    "# c_betas = np.dot(contrast_vector, betas)\n",
    "\n",
    "# N = mean_timecourses.shape[0]\n",
    "# P = betas.shape[0]\n",
    "# df = (N - P)\n",
    "\n",
    "# sigma_hat = np.sum((mean_timecourses.T - yhat) ** 2, axis=1) / df\n",
    "# des_var = contrast_vector.dot(np.linalg.pinv(dm.dot(dm.T))).dot(contrast_vector.T)\n",
    "\n",
    "# t = c_betas / np.sqrt(sigma_hat * des_var)\n",
    "# # t[np.abs(t) < 4] = t[np.abs(t) < 4]/4\n",
    "\n",
    "# surf_t = np.r_[t[li], t[ri]]\n",
    "\n",
    "\n",
    "# f2 = cx.quickshow(cx.Vertex(surf_t, subject='hcp_999999', cmap='CyanBlueGrayRedPink', vmin=-10, vmax=10), height=2048);\n",
    "# # f2.suptitle('left hand vs right hand contrast');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf_hcp",
   "language": "python",
   "name": "cf_hcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
